{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Acquisition\n",
    "\n",
    "Before you can conduct data analysis you must first get hold of the data you want to analyse!\n",
    "\n",
    "Here's a link to the official [Input/Output](http://pandas.pydata.org/pandas-docs/stable/reference/io.html) docs which can be found alongside other comprehensive Pandas docs at [http://pandas.pydata.org/](http://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use Pandas in your script or notebook, you first need to import it into your script\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Start with a dict\n",
    "\n",
    "Dicts are a core type of data for Python and an easy way of working with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict = [{ \"ColA\": 100, \"ColB\": \"Mercedes\"}\n",
    "          , { \"ColA\": 200, \"ColB\": \"VW\"}\n",
    "          , { \"ColA\": 300, \"ColB\": \"BMW\"}\n",
    "          , { \"ColA\": 400, \"ColB\": \"Honda\"}\n",
    "         ]\n",
    "\n",
    "df = pd.DataFrame(myDict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but manually typing in datasets, or copying and pasting will only go so far!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read a Csv or Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/irisCsv.csv\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read an Excel File\n",
    "\n",
    "It is estmated that an huge proportion of the world's **actionable** data and decisions are held in Excel, more than any other format. This makes it a crucial source to be able to read from.\n",
    "\n",
    "See also - [openpyxl](\"https://pypi.org/project/openpyxl/\") for cleaning/transforming spreadsheet data e.g. removing headers, footers, surplus columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/irisExcel.xlsx\", sheet_name=\"Sheet1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read From Relational Databases e.g. SQL Server\n",
    "\n",
    "SQL Server, MySQL, Postgres etc require a connection manager to be setup first. I'll be using MS SQL Server and the tried-and-trusted \"pyodbc\" library (https://pypi.org/project/pyodbc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect(\"Driver={ODBC Driver 17 for SQL Server};Server=.\\SQL2017;Database=AdventureWorks2017;Trusted_Connection=Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection established, we can now run SQL Queries with relative ease...\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT TOP 100 * FROM Person.Person\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and define an \"index\" column...\n",
    "df = pd.read_sql(\"SELECT TOP 100 * FROM Person.Person\", conn, index_col=\"BusinessEntityID\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scrape a Website using Requests+Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of work is suprisingly common in Data Engineering does need some extra libraries first. These two \"pip install\" commands will get what you need; you may need to swap \"pip\" for \"pip3\" if installing on linux. \n",
    "\n",
    "Requests - a library for interacting with web pages over html\n",
    "\"Beautiful Soup 4\" - a library for parsing HTML (or XML) trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example - extract the league table from the Complete University Guide's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.thecompleteuniversityguide.co.uk/league-tables/rankings\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(page.content)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and finally, to Pandas...\n",
    "\n",
    "df = pd.read_html(str(soup.find_all(\"table\")[0]))[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Requests library is great for GET, POST, PUT, PATCH, DELETE etc, but struggles at scraping SPAs - loosely speaking, it fails if the content can be modified by javascript without reloading the page, or if the changed content cannot be accessed by URL alone.\n",
    "\n",
    "Luckily there is an alternative for SPA pages in the form of [Selenium](https://pypi.org/project/selenium/) (PLUS a Firefox or Chrome Webdriver...) but you'll still probably want to parse the page content using BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read from a Parquet File\n",
    "\n",
    "Columnar file storage format closely associated with HDFS and data lakes. Best suited to large volumes of data.\n",
    "\n",
    "https://parquet.apache.org/documentation/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/irisParquet.parquet\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how I copied and pasted the CSV reader and simply swapped the format to \"parquet\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Builtin Datasets\n",
    "\n",
    "Some libraries, such as **SciKitLearn** (aka *\"sklearn\"*) come with builtin datasets for training and demonstration purposes. This takes the guesswork out of acquisition for demo purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import the iris example dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the data isn't 100% ready for Pandas - the following code would error:\n",
    "\n",
    "    df = pd.DataFrame(iris)\n",
    "    \n",
    "...but we can wrangle the data into shape easily enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the data is piece by piece\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df[\"Species\"] = [ iris.target_names[x] for x in iris.target ]\n",
    "\n",
    "# Show the start of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
